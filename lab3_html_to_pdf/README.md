# Лабораторная работа №3: Парсинг HTML и конвертация в PDF

## Цель

Разработать инструмент для парсинга HTML-страниц с сайта выбранной предметной области, извлечения полезного контента и сохранения его в формате PDF.

## Задачи

1. Выбрать предметную область и найти сайт-представитель.
2. Получить список URL-адресов страниц сайта (через sitemap.xml или другим способом).
3. Автоматически определять и извлекать информативные страницы.
4. Сохранять извлеченный контент в форматах HTML, TXT и PDF.

## Требования

- Python 3.7 или выше
- Библиотеки: requests, beautifulsoup4, weasyprint
- Точность извлечения контента не менее 90%
- Обработка минимум 15-20 информативных страниц

## Рекомендации по реализации

1. **Использование карты сайта**: Начните с загрузки и парсинга sitemap.xml, если доступно.
2. **Сортировка URL по глубине**: URL с большим количеством сегментов в пути обычно содержат более информативные страницы.
3. **Оценка информативности**: Используйте критерии оценки страниц, например:
   - Количество слов (более 300)
   - Соотношение текста к ссылкам (много текста и мало ссылок)
   - Наличие заголовков и структурированного контента
4. **Выделение контента**: Адаптируйте селекторы BeautifulSoup под структуру конкретного сайта.
5. **Обработка кодировки**: Особое внимание уделите кириллице и специальным символам.

## Пример предметных областей и сайтов

1. **Банки и финансы**
   - Тинькофф Банк (tinkoff.ru)
   - Сбербанк (sberbank.ru)
   - Банк ВТБ (vtb.ru)

2. **Медицина и здравоохранение**
   - ЕМИАС (emias.info)
   - Здоровье Mail.ru (health.mail.ru)
   - Медицинский портал MedPortal (medportal.ru)

3. **Государственные услуги**
   - Госуслуги (gosuslugi.ru)
   - ФНС России (nalog.gov.ru)
   - Пенсионный фонд РФ (pfr.gov.ru)

4. **Образование**
   - Stepik (stepik.org)
   - Открытое образование (openedu.ru)
   - Нетология (netology.ru)

5. **Туризм**
   - Туту.ру (tutu.ru)
   - Травелата (travelata.ru)
   - Трипстер (tripster.ru)

## Структура проекта

```
lab3_html_parsing/
├── output/
│   ├── html/         # Сохраненные HTML-страницы
│   ├── pdf/          # Конвертированные PDF-документы
│   └── text/         # Извлеченный текст
├── html_parser.ipynb # Jupyter Notebook с кодом парсера
└── requirements.txt  # Зависимости проекта
```

## Критерии оценки

1. **Корректность извлечения контента (40%)**
   - Отсутствие в тексте навигационных элементов, рекламы, скриптов
   - Сохранение основного информационного наполнения страницы

2. **Качество PDF-документов (20%)**
   - Читаемость и форматирование
   - Сохранение структуры документа (заголовки, параграфы)

3. **Эффективность обхода сайта (20%)**
   - Правильный выбор информативных страниц
   - Оптимизация запросов (задержки, обработка ошибок)

4. **Полнота решения (20%)**
   - Количество обработанных страниц (минимум 15-20)
   - Разнообразие контента

## Советы для решения типичных проблем

- **Блокировка запросов**: Используйте User-Agent в заголовках и добавляйте задержки между запросами
- **Кодировка текста**: Правильно определяйте кодировку из заголовков или meta-тегов
- **Разбор сложных HTML**: Используйте комбинацию BeautifulSoup и регулярных выражений

## Результаты

Готовый набор из 15-20 PDF-документов будет использован в следующей лабораторной работе для создания векторной базы данных и построения системы поиска информации на основе RAG (Retrieval-Augmented Generation).