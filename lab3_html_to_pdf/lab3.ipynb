{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №3: Парсинг HTML и конвертация в PDF\n",
    "\n",
    "В этой лабораторной работе мы разработаем инструмент для:\n",
    "1. Извлечения страниц с полезной информацией с банковского сайта\n",
    "2. Очистки и форматирования контента с помощью BeautifulSoup\n",
    "3. Сохранения результатов в формате PDF с помощью WeasyPrint\n",
    "\n",
    "Такой подход позволит нам собрать справочную информацию с сайта банка в удобном для чтения формате."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Подготовка рабочей среды и импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка необходимых библиотек\n",
    "!pip install requests beautifulsoup4 lxml weasyprint # docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Директории для сохранения файлов созданы успешно\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import weasyprint\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime\n",
    "\n",
    "# Настройка путей для сохранения данных\n",
    "OUTPUT_DIRECTORY = 'bank_data_output'\n",
    "HTML_DIRECTORY = os.path.join(OUTPUT_DIRECTORY, 'html')\n",
    "PDF_DIRECTORY = os.path.join(OUTPUT_DIRECTORY, 'pdf')\n",
    "TEXT_DIRECTORY = os.path.join(OUTPUT_DIRECTORY, 'text')\n",
    "\n",
    "# Создаем все необходимые директории\n",
    "for directory in [OUTPUT_DIRECTORY, HTML_DIRECTORY, PDF_DIRECTORY, TEXT_DIRECTORY]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "print(\"Директории для сохранения файлов созданы успешно\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Получение списка URL из карты сайта (sitemap.xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено 469 URL из карты сайта.\n"
     ]
    }
   ],
   "source": [
    "def get_urls_from_sitemap(sitemap_url):\n",
    "    \"\"\"\n",
    "    Получает список URL из sitemap.xml\n",
    "    \n",
    "    Args:\n",
    "        sitemap_url (str): URL карты сайта\n",
    "        \n",
    "    Returns:\n",
    "        list: Список URL из карты сайта\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(sitemap_url)\n",
    "        response.raise_for_status()\n",
    "        # Используем lxml парсер специально для XML\n",
    "        sitemap_soup = BeautifulSoup(response.content, 'lxml-xml')\n",
    "        # Извлекаем все URL из тега <loc>\n",
    "        urls = [loc.text for loc in sitemap_soup.find_all('loc')]\n",
    "        print(f'Найдено {len(urls)} URL из карты сайта.')\n",
    "        return urls\n",
    "    except Exception as e:\n",
    "        print(f'Ошибка при загрузке карты сайта: {e}')\n",
    "        # Резервный список URL, если карта сайта недоступна\n",
    "        backup_urls = [\n",
    "            \"https://www.tbank.ru/business/\",\n",
    "            \"https://www.tbank.ru/business/help/\",\n",
    "            \"https://www.tbank.ru/business/cards/\"\n",
    "        ]\n",
    "        print(f'Используем резервный список из {len(backup_urls)} URL')\n",
    "        return backup_urls\n",
    "\n",
    "# Получаем список URL из карты сайта\n",
    "BANK_SITEMAP_URL = 'https://www.tbank.ru/business/help/sitemap.xml'\n",
    "all_website_urls = get_urls_from_sitemap(BANK_SITEMAP_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Загрузка HTML-страниц и обработка кодировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_webpage(url):\n",
    "    \"\"\"\n",
    "    Загружает HTML-страницу и корректно обрабатывает кодировку\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL страницы для загрузки\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (имя_файла, содержимое_html) или (имя_файла, None) при ошибке\n",
    "    \"\"\"\n",
    "    print(f\"Загрузка страницы: {url}\")\n",
    "    \n",
    "    try:\n",
    "        # Добавляем заголовки для имитации браузера и правильной обработки кодировки\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'ru,en-US;q=0.7,en;q=0.3',\n",
    "            'Accept-Encoding': 'gzip, deflate, br'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()  # Вызывает исключение при ошибке HTTP\n",
    "        \n",
    "        # Определяем кодировку страницы\n",
    "        if response.encoding.upper() == 'ISO-8859-1':\n",
    "            # Requests иногда неправильно определяет кодировку\n",
    "            encoding = None\n",
    "            # Проверяем кодировку в заголовке Content-Type\n",
    "            content_type = response.headers.get('Content-Type', '')\n",
    "            charset_match = re.search(r'charset=([\\w-]+)', content_type)\n",
    "            if charset_match:\n",
    "                encoding = charset_match.group(1)\n",
    "            \n",
    "            if not encoding:\n",
    "                # Ищем кодировку в meta-теге HTML\n",
    "                meta_match = re.search(r'<meta[^>]*charset=[\"\\']*([^\\\"\\'>]+)', response.text, re.IGNORECASE)\n",
    "                if meta_match:\n",
    "                    encoding = meta_match.group(1)\n",
    "            \n",
    "            # Устанавливаем найденную кодировку или UTF-8 по умолчанию\n",
    "            if encoding:\n",
    "                response.encoding = encoding\n",
    "            else:\n",
    "                response.encoding = 'utf-8'\n",
    "        \n",
    "        # Генерируем имя файла из URL\n",
    "        url_parts = urlparse(url)\n",
    "        path = url_parts.path.rstrip('/')\n",
    "        if path:\n",
    "            filename = os.path.basename(path) or url_parts.netloc.replace('.', '_')\n",
    "        else:\n",
    "            filename = url_parts.netloc.replace('.', '_')\n",
    "            \n",
    "        # Добавляем .html, если нет расширения\n",
    "        if '.' not in filename:\n",
    "            filename += '.html'\n",
    "        \n",
    "        # Сохраняем HTML в файл\n",
    "        filepath = os.path.join(HTML_DIRECTORY, filename)\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "            \n",
    "        print(f\"HTML сохранен в {filepath}\")\n",
    "        return filename, response.text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке {url}: {e}\")\n",
    "        if 'url_parts' in locals() and 'filename' in locals():\n",
    "            return filename, None\n",
    "        else:\n",
    "            # Если произошла ошибка до создания имени файла, генерируем его из URL\n",
    "            parsed_url = urlparse(url)\n",
    "            path = parsed_url.path.rstrip('/')\n",
    "            if path:\n",
    "                filename = os.path.basename(path) or parsed_url.netloc.replace('.', '_')\n",
    "            else:\n",
    "                filename = parsed_url.netloc.replace('.', '_')\n",
    "                \n",
    "            if '.' not in filename:\n",
    "                filename += '.html'\n",
    "                \n",
    "            return filename, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Определение информативности страницы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_content_page(html_content):\n",
    "    \"\"\"\n",
    "    Определяет, является ли страница информативной (содержит полезный контент)\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): HTML-содержимое страницы\n",
    "        \n",
    "    Returns:\n",
    "        bool: True, если страница содержит полезную информацию\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Удаляем скрипты и стили для точного подсчета текста\n",
    "        for tag in ['script', 'style']:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "        \n",
    "        # Получаем текст страницы\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        # Аналитика страницы\n",
    "        word_count = len(text.split())\n",
    "        link_count = len(soup.find_all('a', href=True))\n",
    "        link_to_text_ratio = link_count / word_count if word_count > 0 else float('inf')\n",
    "        \n",
    "        # Критерии информативной страницы\n",
    "        is_informative = (\n",
    "            # Много текста\n",
    "            word_count > 300 or\n",
    "            # Среднее количество текста с малым количеством ссылок\n",
    "            (word_count > 100 and link_count < 15) or\n",
    "            # Хорошее соотношение текста к ссылкам\n",
    "            (word_count > 150 and link_to_text_ratio < 0.1)\n",
    "        )\n",
    "        \n",
    "        if is_informative:\n",
    "            print(f\"✅ Информативная страница: {word_count} слов, {link_count} ссылок\")\n",
    "        else:\n",
    "            print(f\"❌ Не информативная страница: {word_count} слов, {link_count} ссылок\")\n",
    "            \n",
    "        return is_informative\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при анализе страницы: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Извлечение основного контента страницы с помощью BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_main_content(html_content, url):\n",
    "    \"\"\"\n",
    "    Извлекает основной контент страницы и форматирует его\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): HTML-содержимое страницы\n",
    "        url (str): URL страницы\n",
    "        \n",
    "    Returns:\n",
    "        str: Отформатированный HTML с основным контентом\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Удаляем ненужные элементы\n",
    "        for tag in ['script', 'style', 'iframe', 'noscript']:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "        \n",
    "        # Ищем основной контент по популярным селекторам\n",
    "        content_selectors = [\n",
    "            'main', 'article', '.content', '#content', '.main-content', \n",
    "            '.page-content', '.container', '.article-content'\n",
    "        ]\n",
    "        \n",
    "        main_content = None\n",
    "        for selector in content_selectors:\n",
    "            content = soup.select_one(selector)\n",
    "            if content and len(content.get_text(strip=True)) > 200:\n",
    "                main_content = content\n",
    "                print(f\"Найден основной контент по селектору: {selector}\")\n",
    "                break\n",
    "                \n",
    "        # Если не нашли контент по селекторам, используем body\n",
    "        if not main_content:\n",
    "            main_content = soup.body\n",
    "            print(\"Используем все содержимое тела страницы (body)\")\n",
    "        \n",
    "        # Получаем заголовок страницы\n",
    "        page_title = soup.title.string if soup.title else 'Банковская информация'\n",
    "        \n",
    "        # Создаем новый HTML-документ с форматированием\n",
    "        formatted_html = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html lang=\"ru\">\n",
    "        <head>\n",
    "            <meta charset=\"UTF-8\">\n",
    "            <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n",
    "            <title>{page_title}</title>\n",
    "            <style>\n",
    "                @charset \"UTF-8\";\n",
    "                body {{ \n",
    "                    font-family: 'Arial', sans-serif; \n",
    "                    line-height: 1.6; \n",
    "                    margin: 30px; \n",
    "                    color: #333; \n",
    "                    max-width: 800px;\n",
    "                    margin: 0 auto;\n",
    "                    padding: 20px;\n",
    "                }}\n",
    "                h1 {{ \n",
    "                    color: #0066cc; \n",
    "                    border-bottom: 1px solid #ddd; \n",
    "                    padding-bottom: 10px; \n",
    "                }}\n",
    "                h2, h3, h4 {{ color: #0066cc; }}\n",
    "                a {{ color: #0066cc; text-decoration: none; }}\n",
    "                a:hover {{ text-decoration: underline; }}\n",
    "                img {{ max-width: 100%; height: auto; }}\n",
    "                table {{ border-collapse: collapse; width: 100%; margin: 15px 0; }}\n",
    "                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "                th {{ background-color: #f2f2f2; }}\n",
    "                .footer {{ \n",
    "                    margin-top: 30px;\n",
    "                    border-top: 1px solid #ddd;\n",
    "                    padding-top: 10px;\n",
    "                    font-size: 12px;\n",
    "                    color: #666;\n",
    "                }}\n",
    "                @media print {{\n",
    "                    body {{ font-size: 12pt; }}\n",
    "                    a {{ text-decoration: none; color: #000; }}\n",
    "                }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>{page_title}</h1>\n",
    "            {main_content}\n",
    "            <div class=\"footer\">\n",
    "                Источник: <a href=\"{url}\">{url}</a><br>\n",
    "                Дата извлечения: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        return formatted_html\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при извлечении контента: {e}\")\n",
    "        return html_content  # Возвращаем исходный HTML в случае ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Извлечение и форматирование текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_content(html_content):\n",
    "    \"\"\"\n",
    "    Извлекает и форматирует текст из HTML\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): HTML-содержимое страницы\n",
    "        \n",
    "    Returns:\n",
    "        str: Извлеченный текст\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Удаляем ненужные элементы\n",
    "        for tag in ['script', 'style', 'iframe', 'noscript']:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "        \n",
    "        # Получаем заголовок\n",
    "        title = soup.title.string if soup.title else ''\n",
    "        formatted_text = f\"{title}\\n{'='*len(title)}\\n\\n\" if title else \"\"\n",
    "        \n",
    "        # Структурированное извлечение текста с сохранением иерархии\n",
    "        # Обрабатываем заголовки и параграфы\n",
    "        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li']):\n",
    "            tag_text = tag.get_text(strip=True)\n",
    "            if tag_text:\n",
    "                if tag.name.startswith('h'):\n",
    "                    # Заголовки выделяем в зависимости от уровня\n",
    "                    level = int(tag.name[1])\n",
    "                    formatted_text += f\"\\n{'#' * level} {tag_text}\\n\"\n",
    "                elif tag.name == 'p':\n",
    "                    # Параграфы с двойным переносом строки\n",
    "                    formatted_text += f\"{tag_text}\\n\\n\"\n",
    "                elif tag.name == 'li':\n",
    "                    # Элементы списка с маркерами\n",
    "                    formatted_text += f\"- {tag_text}\\n\"\n",
    "        \n",
    "        # Если структурированное извлечение дало мало текста,\n",
    "        # используем обычное извлечение текста\n",
    "        if len(formatted_text) < 200:\n",
    "            formatted_text = title + \"\\n\\n\" if title else \"\"\n",
    "            formatted_text += soup.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        # Очистка текста от лишних пробелов и переносов строк\n",
    "        formatted_text = re.sub(r'\\n{3,}', '\\n\\n', formatted_text)  # Удаление лишних переносов\n",
    "        formatted_text = re.sub(r'\\s{2,}', ' ', formatted_text)     # Удаление лишних пробелов\n",
    "        \n",
    "        # Добавляем информацию об источнике в конец текста\n",
    "        formatted_text += \"\\n\\n----------\\n\"\n",
    "        formatted_text += f\"Дата извлечения: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "        \n",
    "        return formatted_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при извлечении текста: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Конвертация HTML в PDF с помощью WeasyPrint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_html_to_pdf(html_content, output_file):\n",
    "    \"\"\"\n",
    "    Конвертирует HTML в PDF с помощью WeasyPrint\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): HTML-содержимое для конвертации\n",
    "        output_file (str): Путь для сохранения PDF\n",
    "        \n",
    "    Returns:\n",
    "        bool: True в случае успеха, False в случае ошибки\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Конвертация HTML в PDF: {output_file}\")\n",
    "        \n",
    "        # Сохраняем HTML во временный файл\n",
    "        temp_html_path = os.path.join(OUTPUT_DIRECTORY, 'temp_convert.html')\n",
    "        with open(temp_html_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        # Конвертируем в PDF с помощью weasyprint\n",
    "        html = weasyprint.HTML(filename=temp_html_path)\n",
    "        html.write_pdf(output_file)\n",
    "        \n",
    "        # Удаляем временный файл\n",
    "        if os.path.exists(temp_html_path):\n",
    "            os.remove(temp_html_path)\n",
    "        \n",
    "        # Проверяем, что PDF был создан\n",
    "        if os.path.exists(output_file) and os.path.getsize(output_file) > 0:\n",
    "            print(f\"PDF успешно создан: {output_file}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Ошибка: PDF файл не создан или имеет нулевой размер\")\n",
    "            return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при конвертации в PDF: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Основная функция обработки URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_webpage(url):\n",
    "    \"\"\"\n",
    "    Полная обработка одной веб-страницы: загрузка, извлечение контента, конвертация в PDF\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL страницы для обработки\n",
    "        \n",
    "    Returns:\n",
    "        bool: True в случае успешной обработки, False в случае ошибки\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Обработка URL: {url} ===\\n\")\n",
    "    \n",
    "    # Шаг 1: Загрузка HTML-страницы\n",
    "    filename, html_content = download_webpage(url)\n",
    "    \n",
    "    if not html_content:\n",
    "        print(\"❌ Не удалось загрузить HTML-страницу\")\n",
    "        return False\n",
    "    \n",
    "    # Шаг 2: Проверка информативности страницы\n",
    "    if not is_content_page(html_content):\n",
    "        print(\"⏩ Страница не содержит полезной информации, пропускаем\")\n",
    "        return False\n",
    "    \n",
    "    # Шаг 3: Извлечение основного контента\n",
    "    formatted_html = extract_main_content(html_content, url)\n",
    "    \n",
    "    # Сохраняем очищенный HTML\n",
    "    clean_html_filename = f\"clean_{filename}\"\n",
    "    clean_html_path = os.path.join(HTML_DIRECTORY, clean_html_filename)\n",
    "    with open(clean_html_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(formatted_html)\n",
    "    print(f\"✅ Очищенный HTML сохранен в {clean_html_path}\")\n",
    "    \n",
    "    # Шаг 4: Извлечение текста\n",
    "    extracted_text = extract_text_content(html_content)\n",
    "    \n",
    "    # Сохраняем извлеченный текст\n",
    "    text_filename = os.path.splitext(filename)[0] + '.txt'\n",
    "    text_path = os.path.join(TEXT_DIRECTORY, text_filename)\n",
    "    with open(text_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(extracted_text)\n",
    "    print(f\"✅ Текст сохранен в {text_path}\")\n",
    "    \n",
    "    # Шаг 5: Конвертация в PDF\n",
    "    pdf_filename = os.path.splitext(filename)[0] + '.pdf'\n",
    "    pdf_path = os.path.join(PDF_DIRECTORY, pdf_filename)\n",
    "    success = convert_html_to_pdf(formatted_html, pdf_path)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"✅ Обработка URL завершена успешно\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"❌ Обработка URL завершена с ошибками\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте добавим код для обработки списка URL и получения данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сортировка URL по длине пути (количеству сегментов)\n",
    "def sort_urls_by_depth(urls):\n",
    "    \"\"\"Сортирует URLs по глубине (количеству сегментов)\"\"\"\n",
    "    def get_path_depth(url):\n",
    "        parsed = urlparse(url)\n",
    "        # Считаем количество сегментов в пути\n",
    "        segments = [s for s in parsed.path.split('/') if s]\n",
    "        return len(segments)\n",
    "    \n",
    "    # Сортируем URL по убыванию глубины\n",
    "    return sorted(urls, key=get_path_depth, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Начало обработки URL из карты сайта ===\n",
      "\n",
      "[1/50] Проверка URL: https://www.tbank.ru/business/help/sales/loans/offline-integration/sell-offline/\n",
      "Загрузка страницы: https://www.tbank.ru/business/help/sales/loans/offline-integration/sell-offline/\n",
      "HTML сохранен в bank_data_output/html/sell-offline.html\n",
      "✅ Информативная страница: 1722 слов, 367 ссылок\n",
      "🔍 Найдена информативная страница #1\n",
      "\n",
      "=== Обработка URL: https://www.tbank.ru/business/help/sales/loans/offline-integration/sell-offline/ ===\n",
      "\n",
      "Загрузка страницы: https://www.tbank.ru/business/help/sales/loans/offline-integration/sell-offline/\n",
      "HTML сохранен в bank_data_output/html/sell-offline.html\n",
      "✅ Информативная страница: 1722 слов, 367 ссылок\n",
      "Найден основной контент по селектору: article\n",
      "✅ Очищенный HTML сохранен в bank_data_output/html/clean_sell-offline.html\n",
      "✅ Текст сохранен в bank_data_output/text/sell-offline.txt\n",
      "Конвертация HTML в PDF: bank_data_output/pdf/sell-offline.pdf\n",
      "PDF успешно создан: bank_data_output/pdf/sell-offline.pdf\n",
      "✅ Обработка URL завершена успешно\n",
      "\n",
      "[2/50] Проверка URL: https://www.tbank.ru/business/help/business-payout/self-employed/payoff/check/\n",
      "Загрузка страницы: https://www.tbank.ru/business/help/business-payout/self-employed/payoff/check/\n",
      "HTML сохранен в bank_data_output/html/check.html\n",
      "✅ Информативная страница: 1150 слов, 343 ссылок\n",
      "🔍 Найдена информативная страница #2\n",
      "\n",
      "=== Обработка URL: https://www.tbank.ru/business/help/business-payout/self-employed/payoff/check/ ===\n",
      "\n",
      "Загрузка страницы: https://www.tbank.ru/business/help/business-payout/self-employed/payoff/check/\n",
      "HTML сохранен в bank_data_output/html/check.html\n",
      "✅ Информативная страница: 1137 слов, 343 ссылок\n",
      "Найден основной контент по селектору: article\n",
      "✅ Очищенный HTML сохранен в bank_data_output/html/clean_check.html\n",
      "✅ Текст сохранен в bank_data_output/text/check.txt\n",
      "Конвертация HTML в PDF: bank_data_output/pdf/check.pdf\n",
      "PDF успешно создан: bank_data_output/pdf/check.pdf\n",
      "✅ Обработка URL завершена успешно\n",
      "\n",
      "[3/50] Проверка URL: https://www.tbank.ru/business/help/business-payments/internet-acquiring/how-work/advanced/\n",
      "Загрузка страницы: https://www.tbank.ru/business/help/business-payments/internet-acquiring/how-work/advanced/\n",
      "HTML сохранен в bank_data_output/html/advanced.html\n",
      "✅ Информативная страница: 2504 слов, 373 ссылок\n",
      "🔍 Найдена информативная страница #3\n",
      "\n",
      "=== Обработка URL: https://www.tbank.ru/business/help/business-payments/internet-acquiring/how-work/advanced/ ===\n",
      "\n",
      "Загрузка страницы: https://www.tbank.ru/business/help/business-payments/internet-acquiring/how-work/advanced/\n",
      "HTML сохранен в bank_data_output/html/advanced.html\n",
      "✅ Информативная страница: 2491 слов, 373 ссылок\n",
      "Найден основной контент по селектору: article\n",
      "✅ Очищенный HTML сохранен в bank_data_output/html/clean_advanced.html\n",
      "✅ Текст сохранен в bank_data_output/text/advanced.txt\n",
      "Конвертация HTML в PDF: bank_data_output/pdf/advanced.pdf\n",
      "PDF успешно создан: bank_data_output/pdf/advanced.pdf\n",
      "✅ Обработка URL завершена успешно\n",
      "\n",
      "[4/50] Проверка URL: https://www.tbank.ru/business/help/registration/registration/register-ip/open/\n",
      "Загрузка страницы: https://www.tbank.ru/business/help/registration/registration/register-ip/open/\n",
      "HTML сохранен в bank_data_output/html/open.html\n",
      "✅ Информативная страница: 1722 слов, 364 ссылок\n",
      "🔍 Найдена информативная страница #4\n",
      "\n",
      "=== Обработка URL: https://www.tbank.ru/business/help/registration/registration/register-ip/open/ ===\n",
      "\n",
      "Загрузка страницы: https://www.tbank.ru/business/help/registration/registration/register-ip/open/\n",
      "HTML сохранен в bank_data_output/html/open.html\n",
      "✅ Информативная страница: 1722 слов, 364 ссылок\n",
      "Найден основной контент по селектору: article\n",
      "✅ Очищенный HTML сохранен в bank_data_output/html/clean_open.html\n",
      "✅ Текст сохранен в bank_data_output/text/open.txt\n",
      "Конвертация HTML в PDF: bank_data_output/pdf/open.pdf\n",
      "PDF успешно создан: bank_data_output/pdf/open.pdf\n",
      "✅ Обработка URL завершена успешно\n",
      "\n",
      "[5/50] Проверка URL: https://www.tbank.ru/business/help/account/currency-ruble/account-replenish/add-money/\n",
      "Загрузка страницы: https://www.tbank.ru/business/help/account/currency-ruble/account-replenish/add-money/\n",
      "HTML сохранен в bank_data_output/html/add-money.html\n",
      "✅ Информативная страница: 2453 слов, 385 ссылок\n",
      "🔍 Найдена информативная страница #5\n",
      "\n",
      "=== Обработка URL: https://www.tbank.ru/business/help/account/currency-ruble/account-replenish/add-money/ ===\n",
      "\n",
      "Загрузка страницы: https://www.tbank.ru/business/help/account/currency-ruble/account-replenish/add-money/\n",
      "HTML сохранен в bank_data_output/html/add-money.html\n",
      "✅ Информативная страница: 2453 слов, 385 ссылок\n",
      "Найден основной контент по селектору: article\n",
      "✅ Очищенный HTML сохранен в bank_data_output/html/clean_add-money.html\n",
      "✅ Текст сохранен в bank_data_output/text/add-money.txt\n",
      "Конвертация HTML в PDF: bank_data_output/pdf/add-money.pdf\n",
      "PDF успешно создан: bank_data_output/pdf/add-money.pdf\n",
      "✅ Обработка URL завершена успешно\n",
      "✅ Достигнуто заданное количество страниц (5)\n",
      "\n",
      "=== Обработка URL завершена ===\n",
      "Всего проверено URL: 5/50\n",
      "Найдено информативных страниц: 5\n",
      "Успешно обработано: 5/5\n",
      "\n",
      "Итог: обработано 5 информативных страниц банковского сайта.\n"
     ]
    }
   ],
   "source": [
    "# 9. Запуск обработки списка URL\n",
    "def process_urls_from_sitemap(max_pages=10):\n",
    "    \"\"\"\n",
    "    Обрабатывает URL из карты сайта до достижения указанного количества\n",
    "    информативных страниц\n",
    "    \n",
    "    Args:\n",
    "        max_pages (int): Максимальное количество страниц для обработки\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Начало обработки URL из карты сайта ===\")\n",
    "    \n",
    "    # Статистика\n",
    "    processed_count = 0    # Всего проверено URL\n",
    "    content_pages = 0      # Найдено информативных страниц\n",
    "    success_count = 0      # Успешно обработано\n",
    "    \n",
    "    # Берем первые 50 URL для проверки\n",
    "    sorted_urls = sort_urls_by_depth(all_website_urls)\n",
    "    urls_to_check = sorted_urls[:50]\n",
    "    \n",
    "    for i, url in enumerate(urls_to_check):\n",
    "        print(f\"\\n[{i+1}/{len(urls_to_check)}] Проверка URL: {url}\")\n",
    "        processed_count += 1\n",
    "        \n",
    "        # Проверяем, является ли страница информативной\n",
    "        filename, html_content = download_webpage(url)\n",
    "        \n",
    "        if not html_content:\n",
    "            print(\"❌ Не удалось загрузить страницу\")\n",
    "            continue\n",
    "            \n",
    "        if is_content_page(html_content):\n",
    "            content_pages += 1\n",
    "            print(f\"🔍 Найдена информативная страница #{content_pages}\")\n",
    "            \n",
    "            # Обрабатываем найденную информативную страницу\n",
    "            if process_webpage(url):\n",
    "                success_count += 1\n",
    "                \n",
    "            # Если достигли лимита страниц, останавливаемся\n",
    "            if content_pages >= max_pages:\n",
    "                print(f\"✅ Достигнуто заданное количество страниц ({max_pages})\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"⏩ Страница не информативна, пропускаем\")\n",
    "            \n",
    "        # Пауза между запросами\n",
    "        time.sleep(1)\n",
    "        \n",
    "    # Выводим итоговую статистику\n",
    "    print(\"\\n=== Обработка URL завершена ===\")\n",
    "    print(f\"Всего проверено URL: {processed_count}/{len(urls_to_check)}\")\n",
    "    print(f\"Найдено информативных страниц: {content_pages}\")\n",
    "    print(f\"Успешно обработано: {success_count}/{content_pages}\")\n",
    "    \n",
    "    return success_count\n",
    "\n",
    "# Запускаем обработку с ограничением в 5 страниц\n",
    "if __name__ == \"__main__\":\n",
    "    num_processed = process_urls_from_sitemap(max_pages=5)\n",
    "    print(f\"\\nИтог: обработано {num_processed} информативных страниц банковского сайта.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом получаем 5 pdf файлов, которые содержат заголовок и ифомрацию со страницы сайта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь попробуем с docling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
