import requests
from bs4 import BeautifulSoup
import os
import time
#import weasyprint
from pyhtml2pdf import converter
import re
from urllib.parse import urlparse
from datetime import datetime

from playwright.async_api import async_playwright

def generate_pdf(html_file, output_pdf):
    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(f'file:{html_file}')
        page.pdf(path=output_pdf)
        browser.close()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
OUTPUT_DIRECTORY = 'bank_data_output'
HTML_DIRECTORY = os.path.join(OUTPUT_DIRECTORY, 'html')
PDF_DIRECTORY = os.path.join(OUTPUT_DIRECTORY, 'pdf')
TEXT_DIRECTORY = os.path.join(OUTPUT_DIRECTORY, 'text')

# 1. –°–æ–∑–¥–∞–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏. –ï—Å–ª–∏ –æ–Ω–∏ —É–∂–µ —Å–æ–∑–¥–∞–Ω—ã —Ç–æ —Ç–æ–∂–µ –≤—Å–µ –æ–∫
def create_output_folders(out_folder, html_folder, pdf_folder, text_folder):
    for directory in [OUTPUT_DIRECTORY, HTML_DIRECTORY, PDF_DIRECTORY, TEXT_DIRECTORY]:
        os.makedirs(directory, exist_ok=True)
    
    print("–î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ —Å–æ–∑–¥–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ")
    return

# 2. –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ URL –∏–∑ –∫–∞—Ä—Ç—ã —Å–∞–π—Ç–∞ (sitemap.xml)
def get_urls_from_sitemap(sitemap_url):
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∏–∑ sitemap.xml
    
    Args:
        sitemap_url (str): URL –∫–∞—Ä—Ç—ã —Å–∞–π—Ç–∞
        
    Returns:
        list: –°–ø–∏—Å–æ–∫ URL –∏–∑ –∫–∞—Ä—Ç—ã —Å–∞–π—Ç–∞
    """
    try:
        response = requests.get(sitemap_url)
        response.raise_for_status()
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º lxml –ø–∞—Ä—Å–µ—Ä —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è XML
        sitemap_soup = BeautifulSoup(response.content, 'lxml-xml')
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ URL –∏–∑ —Ç–µ–≥–∞ <loc>
        urls = [loc.text for loc in sitemap_soup.find_all('loc')]
        print(f'–ù–∞–π–¥–µ–Ω–æ {len(urls)} URL –∏–∑ –∫–∞—Ä—Ç—ã —Å–∞–π—Ç–∞.')
        return urls
    except Exception as e:
        print(f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫–∞—Ä—Ç—ã —Å–∞–π—Ç–∞: {e}')
        # –†–µ–∑–µ—Ä–≤–Ω—ã–π —Å–ø–∏—Å–æ–∫ URL, –µ—Å–ª–∏ –∫–∞—Ä—Ç–∞ —Å–∞–π—Ç–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞
        backup_urls = [
            "https://www.tbank.ru/business/",
            "https://www.tbank.ru/business/help/",
            "https://www.tbank.ru/business/cards/"
        ]
        print(f'–ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏–∑ {len(backup_urls)} URL')
        return backup_urls

# 3. –ó–∞–≥—Ä—É–∑–∫–∞ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
def download_webpage(url):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–æ–¥–∏—Ä–æ–≤–∫—É
    
    Args:
        url (str): URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        
    Returns:
        tuple: (–∏–º—è_—Ñ–∞–π–ª–∞, —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ_html) –∏–ª–∏ (–∏–º—è_—Ñ–∞–π–ª–∞, None) –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
    
    try:
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞ –∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ru,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()  # –í—ã–∑—ã–≤–∞–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ HTTP
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if response.encoding.upper() == 'ISO-8859-1':
            # Requests –∏–Ω–æ–≥–¥–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–¥–∏—Ä–æ–≤–∫—É
            encoding = None
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ Content-Type
            content_type = response.headers.get('Content-Type', '')
            charset_match = re.search(r'charset=([\w-]+)', content_type)
            if charset_match:
                encoding = charset_match.group(1)
            
            if not encoding:
                # –ò—â–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É –≤ meta-—Ç–µ–≥–µ HTML
                meta_match = re.search(r'<meta[^>]*charset=["\']*([^\"\'>]+)', response.text, re.IGNORECASE)
                if meta_match:
                    encoding = meta_match.group(1)
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—É—é –∫–æ–¥–∏—Ä–æ–≤–∫—É –∏–ª–∏ UTF-8 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            if encoding:
                response.encoding = encoding
            else:
                response.encoding = 'utf-8'
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL
        url_parts = urlparse(url)
        path = url_parts.path.rstrip('/')
        if path:
            filename = os.path.basename(path) or url_parts.netloc.replace('.', '_')
        else:
            filename = url_parts.netloc.replace('.', '_')
            
        # –î–æ–±–∞–≤–ª—è–µ–º .html, –µ—Å–ª–∏ –Ω–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
        if '.' not in filename:
            filename += '.html'
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –≤ —Ñ–∞–π–ª
        filepath = os.path.join(HTML_DIRECTORY, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(response.text)
            
        print(f"HTML —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {filepath}")
        return filename, response.text
    
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {e}")
        if 'url_parts' in locals() and 'filename' in locals():
            return filename, None
        else:
            # –ï—Å–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –¥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –µ–≥–æ –∏–∑ URL
            parsed_url = urlparse(url)
            path = parsed_url.path.rstrip('/')
            if path:
                filename = os.path.basename(path) or parsed_url.netloc.replace('.', '_')
            else:
                filename = parsed_url.netloc.replace('.', '_')
                
            if '.' not in filename:
                filename += '.html'
                
            return filename, None

# 4. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
def is_content_page(html_content):
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–π (—Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–µ–∑–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç)
    
    Args:
        html_content (str): HTML-—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        
    Returns:
        bool: True, –µ—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–µ–∑–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    """
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–µ–∫—Å—Ç–∞
        for tag in ['script', 'style']:
            for element in soup.find_all(tag):
                element.decompose()
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        text = soup.get_text(separator=' ', strip=True)
        
        # –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        word_count = len(text.split())
        link_count = len(soup.find_all('a', href=True))
        link_to_text_ratio = link_count / word_count if word_count > 0 else float('inf')
        
        # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        is_informative = (
            # –ú–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            word_count > 300 or
            # –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞ —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å—Å—ã–ª–æ–∫
            (word_count > 100 and link_count < 15) or
            # –•–æ—Ä–æ—à–µ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∫ —Å—Å—ã–ª–∫–∞–º
            (word_count > 150 and link_to_text_ratio < 0.1)
        )
        
        if is_informative:
            print(f"‚úÖ –ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {word_count} —Å–ª–æ–≤, {link_count} —Å—Å—ã–ª–æ–∫")
        else:
            print(f"‚ùå –ù–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {word_count} —Å–ª–æ–≤, {link_count} —Å—Å—ã–ª–æ–∫")
            
        return is_informative
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        return False
    
# 5. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–º–æ—â—å—é BeautifulSoup
def extract_main_content(html_content, url):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –µ–≥–æ
    
    Args:
        html_content (str): HTML-—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        url (str): URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        
    Returns:
        str: –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π HTML —Å –æ—Å–Ω–æ–≤–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
    """
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for tag in ['script', 'style', 'iframe', 'noscript']:
            for element in soup.find_all(tag):
                element.decompose()
        
        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
        content_selectors = [
            'main', 'article', '.content', '#content', '.main-content', 
            '.page-content', '.container', '.article-content'
        ]
        
        main_content = None
        for selector in content_selectors:
            content = soup.select_one(selector)
            if content and len(content.get_text(strip=True)) > 200:
                main_content = content
                print(f"–ù–∞–π–¥–µ–Ω –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É: {selector}")
                break
                
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º body
        if not main_content:
            main_content = soup.body
            print("–ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–µ–ª–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (body)")
        
        # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        page_title = soup.title.string if soup.title else '–ë–∞–Ω–∫–æ–≤—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è'
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π HTML-–¥–æ–∫—É–º–µ–Ω—Ç —Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        formatted_html = f"""
        <!DOCTYPE html>
        <html lang="ru">
        <head>
            <meta charset="UTF-8">
            <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
            <title>{page_title}</title>
            <style>
                @charset "UTF-8";
                body {{ 
                    font-family: 'Arial', sans-serif; 
                    line-height: 1.6; 
                    margin: 30px; 
                    color: #333; 
                    max-width: 800px;
                    margin: 0 auto;
                    padding: 20px;
                }}
                h1 {{ 
                    color: #0066cc; 
                    border-bottom: 1px solid #ddd; 
                    padding-bottom: 10px; 
                }}
                h2, h3, h4 {{ color: #0066cc; }}
                a {{ color: #0066cc; text-decoration: none; }}
                a:hover {{ text-decoration: underline; }}
                img {{ max-width: 100%; height: auto; }}
                table {{ border-collapse: collapse; width: 100%; margin: 15px 0; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
                .footer {{ 
                    margin-top: 30px;
                    border-top: 1px solid #ddd;
                    padding-top: 10px;
                    font-size: 12px;
                    color: #666;
                }}
                @media print {{
                    body {{ font-size: 12pt; }}
                    a {{ text-decoration: none; color: #000; }}
                }}
            </style>
        </head>
        <body>
            <h1>{page_title}</h1>
            {main_content}
            <div class="footer">
                –ò—Å—Ç–æ—á–Ω–∏–∫: <a href="{url}">{url}</a><br>
                –î–∞—Ç–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
            </div>
        </body>
        </html>
        """
        
        return formatted_html
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
        return html_content  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π HTML –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏

# 6. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
def extract_text_content(html_content):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ HTML
    
    Args:
        html_content (str): HTML-—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        
    Returns:
        str: –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for tag in ['script', 'style', 'iframe', 'noscript']:
            for element in soup.find_all(tag):
                element.decompose()
        
        # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        title = soup.title.string if soup.title else ''
        formatted_text = f"{title}\n{'='*len(title)}\n\n" if title else ""
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–µ—Ä–∞—Ä—Ö–∏–∏
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li']):
            tag_text = tag.get_text(strip=True)
            if tag_text:
                if tag.name.startswith('h'):
                    # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –≤—ã–¥–µ–ª—è–µ–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Ä–æ–≤–Ω—è
                    level = int(tag.name[1])
                    formatted_text += f"\n{'#' * level} {tag_text}\n"
                elif tag.name == 'p':
                    # –ü–∞—Ä–∞–≥—Ä–∞—Ñ—ã —Å –¥–≤–æ–π–Ω—ã–º –ø–µ—Ä–µ–Ω–æ—Å–æ–º —Å—Ç—Ä–æ–∫–∏
                    formatted_text += f"{tag_text}\n\n"
                elif tag.name == 'li':
                    # –≠–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–∞ —Å –º–∞—Ä–∫–µ—Ä–∞–º–∏
                    formatted_text += f"- {tag_text}\n"
        
        # –ï—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–ª–æ –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞,
        # –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        if len(formatted_text) < 200:
            formatted_text = title + "\n\n" if title else ""
            formatted_text += soup.get_text(separator='\n', strip=True)
        
        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫
        formatted_text = re.sub(r'\n{3,}', '\n\n', formatted_text)  # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø–µ—Ä–µ–Ω–æ—Å–æ–≤
        formatted_text = re.sub(r'\s{2,}', ' ', formatted_text)     # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–µ –≤ –∫–æ–Ω–µ—Ü —Ç–µ–∫—Å—Ç–∞
        formatted_text += "\n\n----------\n"
        formatted_text += f"–î–∞—Ç–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        
        return formatted_text
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
        return ""
    
# 7. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è HTML –≤ PDF —Å –ø–æ–º–æ—â—å—é WeasyPrint
def convert_html_to_pdf(html_content, output_file):
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç HTML –≤ PDF —Å –ø–æ–º–æ—â—å—é WeasyPrint
    
    Args:
        html_content (str): HTML-—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
        output_file (str): –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è PDF
        
    Returns:
        bool: True –≤ —Å–ª—É—á–∞–µ —É—Å–ø–µ—Ö–∞, False –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    """
    try:
        print(f"–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è HTML –≤ PDF: {output_file}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        #temp_html_path = os.path.join(OUTPUT_DIRECTORY, 'temp_convert.html')
        temp_html_path = os.path.abspath('temp_convert.html')
        with open(temp_html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ PDF —Å –ø–æ–º–æ—â—å—é weasyprint
        generate_pdf(temp_html_path, output_file)

        #converter.convert(f'file:{temp_html_path}', output_file)
        #html = weasyprint.HTML(filename=temp_html_path)
        #html.write_pdf(output_file)
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        if os.path.exists(temp_html_path):
            os.remove(temp_html_path)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ PDF –±—ã–ª —Å–æ–∑–¥–∞–Ω
        if os.path.exists(output_file) and os.path.getsize(output_file) > 0:
            print(f"PDF —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω: {output_file}")
            return True
        else:
            print(f"–û—à–∏–±–∫–∞: PDF —Ñ–∞–π–ª –Ω–µ —Å–æ–∑–¥–∞–Ω –∏–ª–∏ –∏–º–µ–µ—Ç –Ω—É–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä")
            return False
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ PDF: {e}")
        return False
    
# 8. –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ URL
def process_webpage(url):
    """
    –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã: –∑–∞–≥—Ä—É–∑–∫–∞, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ PDF
    
    Args:
        url (str): URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
    Returns:
        bool: True –≤ —Å–ª—É—á–∞–µ —É—Å–ø–µ—à–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏, False –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    """
    print(f"\n=== –û–±—Ä–∞–±–æ—Ç–∫–∞ URL: {url} ===\n")
    
    # –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—ã
    filename, html_content = download_webpage(url)
    
    if not html_content:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—É")
        return False
    
    # –®–∞–≥ 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    if not is_content_page(html_content):
        print("‚è© –°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–µ–∑–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        return False
    
    # –®–∞–≥ 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    formatted_html = extract_main_content(html_content, url)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π HTML
    clean_html_filename = f"clean_{filename}"
    clean_html_path = os.path.join(HTML_DIRECTORY, clean_html_filename)
    with open(clean_html_path, 'w', encoding='utf-8') as f:
        f.write(formatted_html)
    print(f"‚úÖ –û—á–∏—â–µ–Ω–Ω—ã–π HTML —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {clean_html_path}")
    
    # –®–∞–≥ 4: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
    extracted_text = extract_text_content(html_content)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    text_filename = os.path.splitext(filename)[0] + '.txt'
    text_path = os.path.join(TEXT_DIRECTORY, text_filename)
    with open(text_path, 'w', encoding='utf-8') as f:
        f.write(extracted_text)
    print(f"‚úÖ –¢–µ–∫—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {text_path}")
    
    # –®–∞–≥ 5: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ PDF
    pdf_filename = os.path.splitext(filename)[0] + '.pdf'
    pdf_path = os.path.join(PDF_DIRECTORY, pdf_filename)
    success = convert_html_to_pdf(formatted_html, pdf_path)
    #success = True

    if success:
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ URL –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        return True
    else:
        print(f"‚ùå –û–±—Ä–∞–±–æ—Ç–∫–∞ URL –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –æ—à–∏–±–∫–∞–º–∏")
        return False

# –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ URL –ø–æ –¥–ª–∏–Ω–µ –ø—É—Ç–∏ (–∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–µ–≥–º–µ–Ω—Ç–æ–≤)
def sort_urls_by_depth(urls):
    """–°–æ—Ä—Ç–∏—Ä—É–µ—Ç URLs –ø–æ –≥–ª—É–±–∏–Ω–µ (–∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–µ–≥–º–µ–Ω—Ç–æ–≤)"""
    def get_path_depth(url):
        parsed = urlparse(url)
        # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤ –ø—É—Ç–∏
        segments = [s for s in parsed.path.split('/') if s]
        return len(segments)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º URL –ø–æ —É–±—ã–≤–∞–Ω–∏—é –≥–ª—É–±–∏–Ω—ã
    return sorted(urls, key=get_path_depth, reverse=True)

# 9. –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ø–∏—Å–∫–∞ URL
def process_urls_from_sitemap(website_urls, max_pages=10):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç URL –∏–∑ –∫–∞—Ä—Ç—ã —Å–∞–π—Ç–∞ –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
    –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
    
    Args:
        max_pages (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    print(f"\n=== –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ URL –∏–∑ –∫–∞—Ä—Ç—ã —Å–∞–π—Ç–∞ ===")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    processed_count = 0    # –í—Å–µ–≥–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ URL
    content_pages = 0      # –ù–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
    success_count = 0      # –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ
    
    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 50 URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    sorted_urls = sort_urls_by_depth(website_urls)
    urls_to_check = sorted_urls[:50]
    
    for i, url in enumerate(urls_to_check):
        print(f"\n[{i+1}/{len(urls_to_check)}] –ü—Ä–æ–≤–µ—Ä–∫–∞ URL: {url}")
        processed_count += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–π
        filename, html_content = download_webpage(url)
        
        if not html_content:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            continue
            
        if is_content_page(html_content):
            content_pages += 1
            print(f"üîç –ù–∞–π–¥–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ #{content_pages}")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            if process_webpage(url):
                success_count += 1
                
            # –ï—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏–º–∏—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
            if content_pages >= max_pages:
                print(f"‚úÖ –î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –∑–∞–¥–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü ({max_pages})")
                break
        else:
            print("‚è© –°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            
        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        time.sleep(1)
        
    # –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print("\n=== –û–±—Ä–∞–±–æ—Ç–∫–∞ URL –∑–∞–≤–µ—Ä—à–µ–Ω–∞ ===")
    print(f"–í—Å–µ–≥–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ URL: {processed_count}/{len(urls_to_check)}")
    print(f"–ù–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: {content_pages}")
    print(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {success_count}/{content_pages}")
    
    return success_count

# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –≤ 5 —Å—Ç—Ä–∞–Ω–∏—Ü
if __name__ == "__main__":
    # –°–æ–∑–¥–∞–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    create_output_folders(OUTPUT_DIRECTORY, HTML_DIRECTORY, PDF_DIRECTORY, TEXT_DIRECTORY)
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ URL –∏–∑ –∫–∞—Ä—Ç—ã —Å–∞–π—Ç–∞
    all_website_urls = get_urls_from_sitemap('https://www.tbank.ru/business/help/sitemap.xml')
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ä–∞–Ω–∏—Ü—É –∑–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π
    num_processed = process_urls_from_sitemap(all_website_urls, max_pages=5)
    print(f"\n–ò—Ç–æ–≥: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {num_processed} –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –±–∞–Ω–∫–æ–≤—Å–∫–æ–≥–æ —Å–∞–π—Ç–∞.")